[{"content":" Background MIR stands for Mid-Level-IR and is an Intermediate representation that sits between Rust HIR and LLVM IR. An excellent source for learning more about MIR is Introducing MIR.\nI am a systems programmer whose prior systems languages are C and C++. As a systems programmer I am always curious to understand the cost of things and to have some idea of how they are implemented internally. Recently I have been learning Rust and have been looking to bootstrap my understanding of Rust semantics. One technique that worked well for me is to look at the MIR emitted by Rust for small snippets of code and try to understand what is going in. Given how readable and explicit MIR is, I found this approach a much faster way of piercing through syntax and implementation to the underlying semantics. Will share some examples that illustrate this process.\nEmpty function fn f() {} fn main() {} translates to\nfn f() -\u0026gt; () { let mut _0: (); bb0: { return; } } fn main() -\u0026gt; () { let mut _0: (); bb0: { return; } } Identity function fn f(x: i32) -\u0026gt; i32 { x } fn main() {} translates to\nfn f(_1: i32) -\u0026gt; i32 { debug x =\u0026gt; _1; let mut _0: i32; bb0: { _0 = _1; return; } } fn main() -\u0026gt; () { let mut _0: (); bb0: { return; } } Variable declaration fn f() { let x = 0; } fn main() {} expands to\nfn f() -\u0026gt; () { let mut _0: (); scope 1 { debug x =\u0026gt; const 0_i32; } bb0: { return; } } fn main() -\u0026gt; () { let mut _0: (); bb0: { return; } } Multiple variables fn f() { let x = 0; let y = 1; } fn main() {} expands to\nfn f() -\u0026gt; () { let mut _0: (); scope 1 { debug x =\u0026gt; const 0_i32; scope 2 { debug y =\u0026gt; const 1_i32; } } bb0: { return; } } fn main() -\u0026gt; () { let mut _0: (); bb0: { return; } } Copying fn f() { let x = 1; let y = x; } fn main() {} expands to\nfn f() -\u0026gt; () { let mut _0: (); scope 1 { debug x =\u0026gt; const 1_i32; let _1: i32; scope 2 { debug y =\u0026gt; _1; } } bb0: { _1 = const 1_i32; return; } } fn main() -\u0026gt; () { let mut _0: (); bb0: { return; } } Immutable borrowing fn f() { let x = 1; let y = \u0026amp;x; } fn main() {} expands to\nfn f() -\u0026gt; () { let mut _0: (); let _1: i32; scope 1 { debug x =\u0026gt; _1; let _2: \u0026amp;i32; scope 2 { debug y =\u0026gt; _2; } } bb0: { _1 = const 1_i32; _2 = \u0026amp;_1; return; } } fn main() -\u0026gt; () { let mut _0: (); bb0: { return; } } Mutable borrowing fn f() { let mut x = 1; let y = \u0026amp;mut x; } fn main() {} expands to\nfn f() -\u0026gt; () { let mut _0: (); let mut _1: i32; scope 1 { debug x =\u0026gt; _1; let _2: \u0026amp;mut i32; scope 2 { debug y =\u0026gt; _2; } } bb0: { _1 = const 1_i32; _2 = \u0026amp;mut _1; return; } } fn main() -\u0026gt; () { let mut _0: (); bb0: { return; } } Mutation via mutable borrow fn f() { let mut x = 1; let y = \u0026amp;mut x; *y = 2; } fn main() {} expands to\nfn f() -\u0026gt; () { let mut _0: (); let mut _1: i32; scope 1 { debug x =\u0026gt; _1; let _2: \u0026amp;mut i32; scope 2 { debug y =\u0026gt; _2; } } bb0: { _1 = const 1_i32; _2 = \u0026amp;mut _1; (*_2) = const 2_i32; return; } } fn main() -\u0026gt; () { let mut _0: (); bb0: { return; } } Explicit drop use std::mem; fn f() -\u0026gt; i32 { let mut x = 1; let y = \u0026amp;mut x; *y = 2; mem::drop(y); let z = \u0026amp;mut x; *z = 5; x } fn main() {} expands to\nfn f() -\u0026gt; i32 { let mut _0: i32; let mut _1: i32; let _3: (); scope 1 { debug x =\u0026gt; _1; let _2: \u0026amp;mut i32; scope 2 { debug y =\u0026gt; _2; let _4: \u0026amp;mut i32; scope 3 { debug z =\u0026gt; _4; } } } bb0: { _1 = const 1_i32; _2 = \u0026amp;mut _1; (*_2) = const 2_i32; _3 = std::mem::drop::\u0026lt;\u0026amp;mut i32\u0026gt;(move _2) -\u0026gt; [return: bb1, unwind continue]; } bb1: { _4 = \u0026amp;mut _1; (*_4) = const 5_i32; _0 = _1; return; } } fn main() -\u0026gt; () { let mut _0: (); bb0: { return; } } ","permalink":"http://localhost:1313/posts/learn-rust-via-mir/","title":"Learn Rust via MIR"},{"content":" Motivation The following questions have been on my mind for some time now:\nWhy has AMD not been able to replicate CUDA? Why is there no GPGPU (general purpose gpu) programming stack that works across AMD\u0026rsquo;s entire product portfolio? Why do AMD SDKs typically only work with a few Distros and Kernel versions? If you squint a bit and think of shaders as general purpose computation, what explains the disparity between AMD being able to run shader computation on GPUs out of the box on practically any unixen with OSS drivers while being unable to do the same for CUDA style compute tasks? After all adds and multiplies and the primitives around loading compiled code into the GPU should be the same in either case right? This series of blog posts will be to get answers to the above questions. We will start small and try to run until we run into unsolveable blockers. Who knows we might get all the way to the end :D\nOur technical goals for this exercise are a. Minimal set of dependencies b. Depend on a open source stack that can be built and replicated out in the open c. Run across various linux distros out of the box using bog standard kernels and setups\nTechnical choices: We need to make the following technical choices for the following questions\nPick tool to compile the users compute kernels We will use clang since it is widely available and since it has the ability to compile C/C++ to AMD GPU binaries\nAlternatives considered: I did start out with the hypothesis that I would try to use GL or Vulkan compute shaders and then use the same codepaths that mesa uses to compile and load GL/Vulkan shader kernels but after a day or two of research I gave up since I do not understand mesa very well.\nPick a language that users will write compute tasks in I picked C since clang supports that very well and since it would be relatively straightforward to see disassembly of compiled C kernel output and try to figure out my missteps.\nAlternatives considered: If mesa had panned out as a choice I would have considered using GLSL or some Kernel language that compiles to SPIRV. Though to be honest on some level I have come to appreciate that not having graphics bits leak into the users mental model is a net positive wrt general purpose computation. I also considered sticking with opencl but chose not to since getting opencl working out of the box on a linux distro has been a hit or miss affair over time.\nPick a tool to upload and run the compiled kernels We will use libhsa to upload and run our kernels. libhsa is a userspace library that sits on stop of the amdkfd kernel driver that is both open source and available in standard kernels out of the box on various distros. libhsa appears to be widely available on all the distros I checked so far (debian, ubuntu, archlinux).\nAlternatives considered Considered using mesa to compile and load code but gave up due to my own lack of knowledge about mesa. Also looked at libdrm which has really nice apis to copy code from host to gpu but appears to lack the bits to launch a buffer object copied into memory as a kernel\nToy problem definition So lets start with a simple problem and try to use an AMD GPU / APU to solve this. We have two arrays a and b and in the ith position of output we would like to store the addition of ith elements of a and b. The C code for this looks like the following\nvoid add_arrays(int* a, int* b, int* output, int num_elements) { for (int i = 0; i \u0026lt; num_elements; i++) { output[i] = a[i] + b[i]; } } Solution: Vector Addition on GPU using libhsa as orchestrator for compute tasks Our GPU kernel looks like this\n__attribute__((visibility(\u0026#34;default\u0026#34;), amdgpu_kernel)) void add_arrays(int* input_a, int* input_b, int* output) { int index = __builtin_amdgcn_workgroup_id_x() * __builtin_amdgcn_workgroup_size_x() + __builtin_amdgcn_workitem_id_x(); output[index] = input_a[index] + input_b[index]; } We compile this kernel using the equivalent of the following bit of shell commands\nclang -fvisibility=default -target amdgcn-amd-amdhsa -mcpu=gfx1103 -c -O3 kernel.c -o kernel.o clang -fvisibility=default -target amdgcn-amd-amdhsa -mcpu=gfx1103 -O3 kernel.o -o kernel.co And then we are able to launch this kernel using code that looks like this -\nint dispatch() { uint16_t header = (HSA_PACKET_TYPE_KERNEL_DISPATCH \u0026lt;\u0026lt; HSA_PACKET_HEADER_TYPE) | (1 \u0026lt;\u0026lt; HSA_PACKET_HEADER_BARRIER) | (HSA_FENCE_SCOPE_SYSTEM \u0026lt;\u0026lt; HSA_PACKET_HEADER_ACQUIRE_FENCE_SCOPE) | (HSA_FENCE_SCOPE_SYSTEM \u0026lt;\u0026lt; HSA_PACKET_HEADER_RELEASE_FENCE_SCOPE); // total dimension uint16_t dim = 1; if (aql_-\u0026gt;grid_size_y \u0026gt; 1) dim = 2; if (aql_-\u0026gt;grid_size_z \u0026gt; 1) dim = 3; aql_-\u0026gt;group_segment_size = group_static_size_; const uint16_t setup = dim \u0026lt;\u0026lt; HSA_KERNEL_DISPATCH_PACKET_SETUP_DIMENSIONS; const uint32_t header32 = header | (setup \u0026lt;\u0026lt; 16); __atomic_store_n(reinterpret_cast\u0026lt;uint32_t *\u0026gt;(aql_), header32, __ATOMIC_RELEASE); hsa_signal_store_relaxed(queue_-\u0026gt;doorbell_signal, static_cast\u0026lt;hsa_signal_value_t\u0026gt;(packet_index_)); return 0; } // ... shortened for clarity struct alignas(16) args_t { int *input_a; int *input_b; int *output; }; auto device_input_a = (int *) engine.alloc_local(num_elements * sizeof(int)); auto device_input_b = (int *) engine.alloc_local(num_elements * sizeof(int)); auto device_output = (int *) engine.alloc_local(num_elements * sizeof(int)); memcpy(device_input_a, input_a.data(), num_elements * sizeof(int)); memcpy(device_input_b, input_b.data(), num_elements * sizeof(int)); args_t args{.input_a = device_input_a, .input_b = device_input_b, .output = device_output}; Engine::KernelDispatchConfig d_param( \u0026#34;kernel.co\u0026#34;, // kernel compiled object name, \u0026#34;add_arrays.kd\u0026#34;, // name of kernel {120, 1, 1}, // grid size {1, 1, 1}, // workgroup size sizeof(args_t) ); rtn = engine.setup_dispatch(\u0026amp;d_param); memcpy(engine.kernarg_address(), \u0026amp;args, sizeof(args)); engine.dispatch(); // Sum of numbers 0..n is n * (n - 1) / 2 // In our case n = 99 - sum would be 99 * 100 / 2 = 4950 // Since we have two arrays each that sum up to 4950, // we expect the sum of those two arrays to be = 2 * 4950 = 9900 std::cout \u0026lt;\u0026lt; \u0026#34;We expected the sum of 2 * sum (0..99) to be :\u0026#34; \u0026lt;\u0026lt; (num_elements - 1) * num_elements \u0026lt;\u0026lt; \u0026#34;. Calculated sum is \u0026#34; \u0026lt;\u0026lt; std::reduce(device_output, device_output + num_elements, 0) \u0026lt;\u0026lt; std::endl; return 0; } Code for this is available at https://github.com/deepankarsharma/hansa.\nConclusion In this post we were able to stay true to our technical goals\nThe code above only depends on clang and libhsa-dev Code works on a variety of distros and kernels Code works on both GPUs and APUs Code works with upstream kernels One thing worth mentioning is that we currently hard code the GPU architecture that the code is being compiled for in the cmake file. Will fix in the near future.\nShout of gratitude to @blu51899890 who very kindly did not say no when I \u0026ldquo;volunteered\u0026rdquo; him to be my mentor through this effort. Thanks for being generous with your time @blu51899890!!\n","permalink":"http://localhost:1313/posts/hsa-vector-add/","title":"GPGPU on AMD: Vector addition kernel using libhsa"},{"content":" We will be trying to compare various different ways of reading a file using Rust. Apart from \u0026#34;wc -l\u0026#34; we will be running each function 10 times using criterion and then picking the mean.\nCode for the following benchmarks lives at Benchmark code for Linux IO using Rust. In the following code BUFFER_SIZE was 8192 and NUM_BUFFERS was 32.\nDetails about the machine Framework 16 with 7840hs and 64 Gigs of RAM. Power plugged in and performance mode enabled. SSD: WD_BLACK SN850X 4000GB. Test using Gnome Disks shows the read speed at 3.6 GB/s (Sample size 1000MB, 100 Samples). Filesystem : btrfs Uname string: Linux fedora 6.8.8-300.fc40.x86_64 #1 SMP PREEMPT_DYNAMIC Sat Apr 27 17:53:31 UTC 2024 x86_64 GNU/Linux Details about the text file Uncompressed size: 22G Number of lines: 200,000,000 Compressed size after btrfs compression (zstd): 5.3G\nFor the impatient: an overview of the results Method Time (seconds) Mmap with AVX512 2.61 Mmap with AVX2 2.64 io_uring with Vectored IO 2.86 Vectored IO 2.89 Mmap 3.43 io_uring 5.26 wc -l (baseline) 8.01 Direct IO 10.56 BufReader without appends 15.94 BufReader with lines().count() 33.50 Benchmark results Interesting observation was that AVX512 was taking 2.61 seconds, file is ~22G and SSD benchmarks show 3.6 GB/s read speed. This means that the file should be read in about 6 seconds. The AVX512 implementation is reading the file at about 8.4 GB/s. What gives? Turns out Fedora uses btrfs which enables zstd compression by default. Actual on disk size can be found using compsize.\nopdroid@box:~/tmp$ sudo compsize data Processed 1 file, 177437 regular extents (177437 refs), 0 inline. Type Perc Disk Usage Uncompressed Referenced TOTAL 24% 5.3G 21G 21G none 100% 32K 32K 32K zstd 24% 5.3G 21G 21G Thanks to these fine folks @alextjensen - for pointing me to sane defaults for BufReader and to compile to the native arch. @aepau2 - for spotting a glaring error in the wc numbers. I had forgotten to drop the cache before measuring with wc. @rflaherty71 - for pointing me to use more buffers which are larger (64 x 64k). @daniel_c0deb0t - for pointing me to use larger buffers. Always a good idea to use some code we did not write as a baseline.\nBaseline: wc -l opdroid@box:~/tmp$ time wc -l data 200000000 data real\t0m8.010s user\t0m0.193s sys\t0m7.591s We reset the file caches using the following command at the end of each function. I am yet to figure out how to use a teardown function in criterion so that this doesnt get counted in the time taken.\n// TODO: move to a teardown function in criterion fn reset_file_caches() { // Execute the command to reset file caches let output = Command::new(\u0026#34;sudo\u0026#34;) .arg(\u0026#34;sh\u0026#34;) .arg(\u0026#34;-c\u0026#34;) .arg(\u0026#34;echo 3 \u0026gt; /proc/sys/vm/drop_caches\u0026#34;) .output() .expect(\u0026#34;Failed to reset file caches\u0026#34;); // Check if the command executed successfully if !output.status.success() { panic!(\u0026#34;Failed to reset file caches: {:?}\u0026#34;, output); } } Method 1: Read the file using BufReader and use reader.lines().count() fn count_newlines_standard(filename: \u0026amp;str) -\u0026gt; Result\u0026lt;usize, std::io::Error\u0026gt; { let file = File::open(filename)?; let reader = BufReader::with_capacity(16 * 1024, file); let newline_count = reader.lines().count(); reset_file_caches(); Ok(newline_count) } This takes about 36.5 seconds on my machine.\nMethod 2: Read the file using BufReader and avoid string appends pub fn count_newlines_standard_non_appending(filename: \u0026amp;str) -\u0026gt; Result\u0026lt;usize, std::io::Error\u0026gt; { let file = File::open(filename)?; let mut reader = BufReader::with_capacity(64 * 1024, file); let mut newline_count = 0; loop { let len = { let buffer = reader.fill_buf()?; if buffer.is_empty() { break; } newline_count += buffer.iter().filter(|\u0026amp;\u0026amp;b| b == b\u0026#39;\\n\u0026#39;).count(); buffer.len() }; reader.consume(len); } reset_file_caches(); Ok(newline_count) } This takes about 15.94 seconds on my machine. This is less than half of the appending version.\nWhen we look at the flamegraph we can verify that the appends are gone.\nMethod 3: Read the file using Direct IO fn count_newlines_direct_io(filename: \u0026amp;str) -\u0026gt; Result\u0026lt;usize, Error\u0026gt; { let mut open_options = File::options(); open_options.read(true).custom_flags(libc::O_DIRECT); let mut file = open_options.open(filename)?; let mut buffer = vec![0; BUFFER_SIZE]; let mut newline_count = 0; loop { let bytes_read = file.read(\u0026amp;mut buffer)?; if bytes_read == 0 { break; } let chunk_newline_count = buffer[..bytes_read].iter().filter(|\u0026amp;\u0026amp;b| b == b\u0026#39;\\n\u0026#39;).count(); newline_count += chunk_newline_count; } reset_file_caches(); Ok(newline_count) } This takes about 35.7 seconds on my machine.\nMethod 4: Read the file using Mmap fn count_newlines_memmap(filename: \u0026amp;str) -\u0026gt; Result\u0026lt;usize, Error\u0026gt; { let file = File::open(filename)?; let mmap = unsafe { Mmap::map(\u0026amp;file)? }; let newline_count = mmap.iter().filter(|\u0026amp;\u0026amp;b| b == b\u0026#39;\\n\u0026#39;).count(); reset_file_caches(); Ok(newline_count) } This takes about 8.3 seconds on my machine.\nMethod 5: Read the file using Mmap and AVX2 unsafe fn count_newlines_memmap_avx2(filename: \u0026amp;str) -\u0026gt; Result\u0026lt;usize, Error\u0026gt; { let file = File::open(filename)?; let mmap = unsafe { Mmap::map(\u0026amp;file)? }; let newline_byte = b\u0026#39;\\n\u0026#39;; let newline_vector = _mm256_set1_epi8(newline_byte as i8); let mut newline_count = 0; let mut ptr = mmap.as_ptr(); let end_ptr = unsafe { ptr.add(mmap.len()) }; while ptr \u0026lt;= end_ptr.sub(32) { let data = unsafe { _mm256_loadu_si256(ptr as *const __m256i) }; let cmp_result = _mm256_cmpeq_epi8(data, newline_vector); let mask = _mm256_movemask_epi8(cmp_result); newline_count += mask.count_ones() as usize; ptr = unsafe { ptr.add(32) }; } // Count remaining bytes let remaining_bytes = end_ptr as usize - ptr as usize; newline_count += mmap[mmap.len() - remaining_bytes..].iter().filter(|\u0026amp;\u0026amp;b| b == newline_byte).count(); reset_file_caches(); Ok(newline_count) } This takes about 2.64 seconds on my machine.\nMethod 6: Read the file using Mmap and AVX512 unsafe fn count_newlines_memmap_avx512(filename: \u0026amp;str) -\u0026gt; Result\u0026lt;usize, Error\u0026gt; { let file = File::open(filename)?; let mmap = unsafe { Mmap::map(\u0026amp;file)? }; let newline_byte = b\u0026#39;\\n\u0026#39;; let newline_vector = _mm512_set1_epi8(newline_byte as i8); let mut newline_count = 0; let mut ptr = mmap.as_ptr(); let end_ptr = unsafe { ptr.add(mmap.len()) }; while ptr \u0026lt;= end_ptr.sub(64) { let data = unsafe { _mm512_loadu_si512(ptr as *const i32) }; let cmp_result = _mm512_cmpeq_epi8_mask(data, newline_vector); newline_count += cmp_result.count_ones() as usize; ptr = unsafe { ptr.add(64) }; } // Count remaining bytes let remaining_bytes = end_ptr as usize - ptr as usize; newline_count += mmap[mmap.len() - remaining_bytes..].iter().filter(|\u0026amp;\u0026amp;b| b == newline_byte).count(); reset_file_caches(); Ok(newline_count) } This takes about 2.61 seconds on my machine.\nMethod 7: Read the file using Vectored IO fn count_newlines_vectored_io(path: \u0026amp;str) -\u0026gt; Result\u0026lt;usize, Error\u0026gt; { let mut file = File::open(path)?; let mut buffers_: Vec\u0026lt;_\u0026gt; = (0..16).map(|_| vec![0; BUFFER_SIZE]).collect(); let mut buffers: Vec\u0026lt;_\u0026gt; = buffers_.iter_mut().map(|buf| io::IoSliceMut::new(buf)).collect(); let mut newline_count = 0; loop { let bytes_read = file.read_vectored(\u0026amp;mut buffers)?; if bytes_read == 0 { break; } // Calculate how many buffers were filled let filled_buffers = bytes_read / BUFFER_SIZE; // Process the fully filled buffers for buf in \u0026amp;buffers[..filled_buffers] { newline_count += buf.iter().filter(|\u0026amp;\u0026amp;b| b == b\u0026#39;\\n\u0026#39;).count(); } // Handle the potentially partially filled last buffer if filled_buffers \u0026lt; buffers.len() { let last_buffer = \u0026amp;buffers[filled_buffers]; let end = bytes_read % BUFFER_SIZE; newline_count += last_buffer[..end].iter().filter(|\u0026amp;\u0026amp;b| b == b\u0026#39;\\n\u0026#39;).count(); } } Ok(newline_count) } This takes about 7.7 seconds on my machine.\nMethod 8: Read the file using io_uring fn count_lines_io_uring(path: \u0026amp;str) -\u0026gt; io::Result\u0026lt;usize\u0026gt; { let file = File::open(path)?; let fd = file.as_raw_fd(); let mut ring = IoUring::new(8)?; let mut line_count = 0; let mut offset = 0; let mut buf = vec![0; 4096]; let mut read_size = buf.len(); loop { let mut sqe = opcode::Read::new(types::Fd(fd), buf.as_mut_ptr(), read_size as _) .offset(offset as _) .build() .user_data(line_count as _); unsafe { ring.submission() .push(\u0026amp;mut sqe) .expect(\u0026#34;submission queue is full\u0026#34;); } ring.submit_and_wait(1)?; let cqe = ring.completion().next().expect(\u0026#34;completion queue is empty\u0026#34;); let bytes_read = cqe.result() as usize; line_count = cqe.user_data() as usize; if bytes_read == 0 { break; } let data = \u0026amp;buf[..bytes_read]; line_count += data.iter().filter(|\u0026amp;\u0026amp;b| b == b\u0026#39;\\n\u0026#39;).count(); offset += bytes_read as u64; read_size = (buf.len() - (offset as usize % buf.len())) as usize; } Ok(line_count) } This takes about 10.5 seconds on my machine.\nMethod 9: Read the file using io_uring with vectored IO fn count_lines_io_uring_vectored(path: \u0026amp;str) -\u0026gt; io::Result\u0026lt;usize\u0026gt; { let file = File::open(path)?; let fd = file.as_raw_fd(); let mut ring = IoUring::new(NUM_BUFFERS as u32)?; let mut line_count = 0; let mut offset = 0; let mut buffers = vec![vec![0; 8192]; NUM_BUFFERS]; let mut iovecs: Vec\u0026lt;iovec\u0026gt; = buffers .iter_mut() .map(|buf| iovec { iov_base: buf.as_mut_ptr() as *mut _, iov_len: buf.len(), }) .collect(); loop { let mut sqe = opcode::Readv::new(types::Fd(fd), iovecs.as_mut_ptr(), iovecs.len() as _) .offset(offset as _) .build() .user_data(0); unsafe { ring.submission() .push(\u0026amp;mut sqe) .expect(\u0026#34;submission queue is full\u0026#34;); } ring.submit_and_wait(1)?; let cqe = ring.completion().next().expect(\u0026#34;completion queue is empty\u0026#34;); let bytes_read = cqe.result() as usize; if bytes_read == 0 { break; } let mut buffer_line_count = 0; let mut remaining_bytes = bytes_read; for buf in \u0026amp;buffers[..iovecs.len()] { let buf_size = buf.len(); let data_size = remaining_bytes.min(buf_size); let data = \u0026amp;buf[..data_size]; buffer_line_count += data.iter().filter(|\u0026amp;\u0026amp;b| b == b\u0026#39;\\n\u0026#39;).count(); remaining_bytes -= data_size; if remaining_bytes == 0 { break; } } line_count += buffer_line_count; offset += bytes_read as u64; } Ok(line_count) } This takes about 7.6 seconds on my machine.\n","permalink":"http://localhost:1313/posts/rust-io/","title":"Towards Fast IO on Linux using Rust"}]